---
title:  "고문헌 번역 프로젝트(2) - 첫번째 번역 모델"
excerpt: "BOAZ ADV PROJECT - vanila seq2seq 모델"

categories:
  - projects
tags:
  - BOAZ
  - project
  - nlp
last_modified_at: 2019-12-23T08:06:00-05:00
---

# 변명:sweat_smile:

프로젝트가 계획대로 되지 않는다. 그래도 봐줄만한 번역 모델을 만드느라 DB구축은 물론이고 데이터셋 관리도 제대로 되지 않고 있다.

그래서 블로그에 진행상황을 기록하지도 못했다. 기억과 코드를 더듬어서 진행상황을 기록해본다.

이전 포스팅에서는 데이터가 너무 큰 관계로 MongoDB 구축해서 효율적으로 데이터를 관리하고 필요한 데이터만 메모리에 올려 진행하려고 했다. 

그러나! 프로젝트에 시간투자를 많이 하지 않은 관계로.... 우선적으로 번역모델만 빠르게 만들어보려 한다.

이번 포스팅에서는, 첫번째 모델인 vanila seq2seq 모델을 적용한 결과를 소개해본다.

# 데이터 소개

데이터는 [국사편찬위원회에서 번역](http://sillok.history.go.kr/main/main.do)한 조선왕조실록의 원본(한자)와 번역본(현대한글)을 사용했다.

# 번역 모델 소개

## Seq2Seq

![seq2seq](https://wikidocs.net/images/page/24996/%EB%8B%A8%EC%96%B4%ED%86%A0%ED%81%B0%EB%93%A4%EC%9D%B4.PNG)

기계 번역 태스크의 가장 기본이라고 할 수 있는 LSTM Seq2Seq 모델을 사용했다. 

[공식 Keras Document](https://keras.io/examples/lstm_seq2seq/) 에서 제공하는 코드를 대부분 참고하여 개발했다.

아주 단순한 모델로, 참고한 코드는 Character-level이었으므로 Embedding layer를 따로 두지 않았다.

하지만 이 프로젝트에서는 고문헌은 한자 하나씩을 토큰화했고, 한글은 형태소 단위로 토큰화했으므로

모든 토큰의 개수만큼 단어벡터 차원을 부여한다면, 너무 고차원이 된다.

그래서 단어의 의미를 담을 Embedding Layer를 추가하고, 발생 빈도가 적은 토큰들은 `<OOV>`(Out of Vocabulary) 토큰으로 하나의 토큰으로 만들었다.

# 결과

**처참한 결과 **:sob:

나름 고문헌같은 느낌은 나는 결과였다. 그러나, 의미를 전혀 파악하고 있지 못하는 모습.

```
# 원문
○丁酉/雨。 前此久旱, 及上卽位, 霈然下雨, 人心大悅。
(비가 내리었다. 이보다 앞서 오랫동안 가물었는데, 임금이 왕위에 오르자 억수같이 비가 내리니, 백성의 마음이 크게 기뻐하였다.)
# 번역결과
비/NNG가/JKS오/VX았/EP다/EF./SF임금/NNG이/JKS여러/MM신하/NNG들/XSN에게/JKB이르/VV기/ETN를/JKO,/SP"/SS내/NP가/JKS장차/MAG친히/MAG가/VV아서/EC보/VV고/EC,/SP또/MAJ말/NNG하/XSV기/ETN를/JKO,/SP내/NP가/JKS장차/MAG 친히/MAG가/VV아서/EC보/VV고/EC,/SP또/MAJ말/NNG
```

```
# 원문
○立義興親軍衛, 罷都摠中外諸軍事府。
(의흥친군위(義興親軍衛)를 설치하고 도총 중외 제군사부(都摠中外諸軍事府)를 폐지하였다.)
# 번역결과
중국/NNP사람/NNG_등/NNB이/JKS오/VV아서/EC고하/VV기/ETN를/JKO,/SP"/SS_의/JKG_에/JKB는/JX_하/XSV아/EC_하/XSV아/EC,/SP_을/JKO_하/XSV아/EC_하/XSV아/EC,/SP그/MM나머지/NNG는/JX모두/MAG다/MAG알/VV지/EC못하/VXㄴ다/EF./SF"/SS하/XSV았/EP다/EF./SFPAD
```

어느 정도는 될 줄 알았는데... 

- `의흥친군위`와 같은 고유명사 문제와
- 모델 자체의 표현력 부족
- 한자의 다중 의미를 잡아내지 못함

위 세 문제가 가장 심각하다고 생각해서

- 고유명사를 하나의 토큰으로 통합
- Transformer 모델 적용

을 통해 해결해보려 한다.

다음 포스팅에서는 Transformer를 적용한 결과를 소개하겠다.